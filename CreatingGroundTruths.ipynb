{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/violetcfelt/oceanfronts/blob/main/CreatingGroundTruths.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2VoV3EEm2NCD",
        "outputId": "b7ea75b8-9087-4cee-e35d-c9f7578fb4ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import cv2 as cv\n",
        "from osgeo import gdal\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "min_dim = 224"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "MYghMGSVnDWG"
      },
      "outputs": [],
      "source": [
        "#HELPER FUNCTIONS\n",
        "\n",
        "def chl(b1,b2,b3):\n",
        "  #chl coeff from NASA, units are mg/m^3\n",
        "  #note that this returns log(chl), which is the input needed for boa\n",
        "  a0 = 0.2412\n",
        "  a1 = -2.0546\n",
        "  a2 = 1.1776\n",
        "  a3 = -0.5538\n",
        "  a4 = -0.4570\n",
        "\n",
        "  chl = np.zeros((min_dim,min_dim))\n",
        "  for i in range(min_dim):\n",
        "    for j in range(min_dim):\n",
        "      blue = max(b1[i][j],b2[i][j])\n",
        "      green = b3[i][j]\n",
        "      chl[i][j] = a0 + a1*(np.log10(blue/green)) + a2*(np.log10(blue/green))**2 + a3*(np.log10(blue/green))**3 + a4*(np.log10(blue/green))**4\n",
        "  return chl\n",
        "\n",
        "def sst(b10):\n",
        "  #sst coeff from Landsat, units are C\n",
        "  b10 = b10*0.00341802+149-273.15\n",
        "  return b10\n",
        "\n",
        "#HELPER FUNCTIONS FOR BOA\n",
        "def median_filter(data):\n",
        "    return np.median(np.ndarray.flatten(data))\n",
        "\n",
        "def peak_max(data, filter_size):\n",
        "    indexer = filter_size // 2\n",
        "    WE = np.argmax(data[indexer])\n",
        "    NS = np.argmax(np.transpose(data)[indexer])\n",
        "    NWSE = np.argmax(data.diagonal())\n",
        "    NESW = np.argmax(data[:,::-1].diagonal())\n",
        "    return {indexer} == {WE,NS,NWSE,NESW}\n",
        "\n",
        "def peak_min(data, filter_size):\n",
        "    indexer = filter_size // 2\n",
        "    WE = np.argmin(data[indexer])\n",
        "    NS = np.argmin(np.transpose(data)[indexer])\n",
        "    NWSE = np.argmin(data.diagonal())\n",
        "    NESW = np.argmin(data[:,::-1].diagonal())\n",
        "    return {indexer} == {WE,NS,NWSE,NESW}\n",
        "\n",
        "def peak_masks(data, filter_size):\n",
        "    peaks = np.zeros(data.shape)\n",
        "    indexer = filter_size // 2\n",
        "    for i in range(data.shape[0]):\n",
        "        for j in range(data.shape[1]):\n",
        "            test = data[i-indexer:i+indexer+1,j-indexer:j+indexer+1]\n",
        "            if test.shape == (filter_size,filter_size):\n",
        "                if peak_max(test,filter_size) or peak_min(test,filter_size):\n",
        "                    peaks[i][j] = 1\n",
        "    return(peaks)\n",
        "\n",
        "def boa(img):\n",
        "    peak_5 = peak_masks(img,5)\n",
        "    peak_3 = peak_masks(img,3)\n",
        "    new_img = np.zeros(img.shape)\n",
        "    for i in range(img.shape[0]):\n",
        "        for j in range(img.shape[1]):\n",
        "            if not peak_5[i][j] and peak_3[i][j]:\n",
        "                new_img[i][j] = median_filter(img[i-1:i+2,j-1:j+2])\n",
        "            else:\n",
        "                new_img[i][j] = img[i][j]\n",
        "    return new_img\n",
        "\n",
        "def create_ground_truth(img):\n",
        "  old_step = img\n",
        "  new_step = boa(img)\n",
        "  while not np.array_equal(old_step,new_step):\n",
        "      old_step = new_step\n",
        "      new_step = boa(old_step)\n",
        "  img = new_step\n",
        "  img = np.float32(img)\n",
        "  gx = cv.Sobel(img, cv.CV_32F, 1, 0, ksize=3) # the x - direction gradient\n",
        "  gy = cv.Sobel(img, cv.CV_32F, 0, 1, ksize=3) # the y - direction gradient\n",
        "  sobel_final = cv.addWeighted(gx, 0.5, gy, 0.5, 0)\n",
        "  return sobel_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "gZqN2Iya3R-3"
      },
      "outputs": [],
      "source": [
        "for file in glob.glob(\"/content/drive/My Drive/test/*.tif\"):\n",
        "  #crop images to same size, create ground truths\n",
        "  dataset = gdal.Open(file)\n",
        "  SR_B1 = dataset.GetRasterBand(1).ReadAsArray()[0:224,0:224]\n",
        "  SR_B2 = dataset.GetRasterBand(2).ReadAsArray()[0:224,0:224]\n",
        "  SR_B3 = dataset.GetRasterBand(3).ReadAsArray()[0:224,0:224]\n",
        "  SR_B4 = dataset.GetRasterBand(4).ReadAsArray()[0:224,0:224]\n",
        "  ST_B10 = dataset.GetRasterBand(5).ReadAsArray()[0:224,0:224]\n",
        "\n",
        "  boa_chl = create_ground_truth(chl(SR_B1,SR_B2,SR_B3))\n",
        "  boa_chl = (np.abs(boa_chl)>0.05).astype(np.int8)\n",
        "  boa_sst = create_ground_truth(sst(ST_B10))\n",
        "  boa_sst = (np.abs(boa_sst)>0.5).astype(np.int8)\n",
        "  output = np.stack([boa_chl,boa_sst], axis=2)\n",
        "\n",
        "  np.save('/content/drive/My Drive/east_cropped/' + file.split(\"/\")[-1][:-4] + '_output.npy', output)\n",
        "  plt.imsave('/content/drive/My Drive/east_cropped/' + file.split(\"/\")[-1][:-4] + '_output_viz.png', boa_chl)\n",
        "\n",
        "  #crop images to same size, scale\n",
        "  B1 = (dataset.GetRasterBand(6).ReadAsArray()[0:224,0:224]/32727.5*255).astype(np.int8)\n",
        "  B2 = (dataset.GetRasterBand(7).ReadAsArray()[0:224,0:224]/32727.5*255).astype(np.int8)\n",
        "  B3 = (dataset.GetRasterBand(8).ReadAsArray()[0:224,0:224]/32727.5*255).astype(np.int8)\n",
        "  B4 = (dataset.GetRasterBand(9).ReadAsArray()[0:224,0:224]/32727.5*255).astype(np.int8)\n",
        "  B10 = (dataset.GetRasterBand(10).ReadAsArray()[0:224,0:224]/32727.5*255).astype(np.int8)\n",
        "  input = np.stack([B1,B2,B3,B4,B10], axis=2)\n",
        "\n",
        "  np.save('/content/drive/My Drive/east_cropped/' + file.split(\"/\")[-1][:-4] + '_input.npy', input)\n",
        "  plt.imsave('/content/drive/My Drive/east_cropped/' + file.split(\"/\")[-1][:-4] + '_input_viz.png', B1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#USE VISUALIZATIONS TO MANUALLY REMOVE IMAGES WITH LAND IN THEM"
      ],
      "metadata": {
        "id": "iNpq5CRJGEBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CreatingGroundTruths.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNHhiF+1Ro2OSuBHgKBGYyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}